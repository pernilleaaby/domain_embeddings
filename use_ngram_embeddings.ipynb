{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c92e1f7",
   "metadata": {},
   "source": [
    "# Use the ngram embeddings\n",
    "In this program we use our pre-made word-embeddings from BERT. In addition to using the direct n-gram to n-gram comparison (gensim most_similar function) we can perform it contextually by extracting the contextual embeddings of a subset of a input-sentence and comparing it with out word-embedding vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aca3c39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy, useful for efficient vector operations\n",
    "import numpy as np\n",
    "\n",
    "# pytorch \n",
    "import torch\n",
    "\n",
    "# transformoer models from huggingface\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "\n",
    "# gensim library\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# natural langauge toolkit\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2120fd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "NGRAMS = 1\n",
    "LAYER = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56499c9",
   "metadata": {},
   "source": [
    "## Load the pre-trained word-embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27e8b9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in ngram embeddings\n",
    "word_model = KeyedVectors.load(\"academic_ngrams_\"+str(NGRAMS)+\".kv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c3948c",
   "metadata": {},
   "source": [
    "## Load Contextual Model\n",
    "The contextual model is not actually necessary to find similar words. But if you want to find a similar word in context, or from a word not in the word-model dictionary we need the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43f85a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3306e71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# define model name\n",
    "model = 'bert-base-uncased' #for norwegian you can use: 'NbAiLab/nb-bert-base' or 'ltgoslo/norbert'\n",
    "\n",
    "# the tokenizer plits the input text into tokens, which in this case is called wordpieces \n",
    "tokenizer = BertTokenizerFast.from_pretrained(model)\n",
    "\n",
    "# download the model online\n",
    "bert_model = BertModel.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d534bcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# change to cuda cores if a gpu is available\n",
    "bert_model.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9445fc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_embeddings(word_tokens): \n",
    "    \"\"\"Obtain token embeddings and token to word token mapping \n",
    "    from BERT-model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sent : array\n",
    "        Input array of word tokens\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        token to word id mapping\n",
    "    list\n",
    "        token embeddings from all layers\n",
    "    \"\"\"\n",
    "    with torch.no_grad(): \n",
    "        inputs = tokenizer(word_tokens, return_tensors = \"pt\", truncation = True, max_length = 512, is_split_into_words=True)\n",
    "        word_ids = inputs.word_ids(batch_index=0)\n",
    "        outputs = bert_model(**inputs.to(device), output_hidden_states=True)\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "        token_embeddings = torch.stack(hidden_states, dim=0) #stack all hidden states into same tensor\n",
    "        token_embeddings = token_embeddings.squeeze(dim=1) # remove empty dimension\n",
    "        token_embeddings = token_embeddings.permute(1,0,2)\n",
    "        \n",
    "    # convert to numpy arrays\n",
    "    word_ids = np.array(word_ids)\n",
    "    token_embeddings = token_embeddings.to(\"cpu\").detach().numpy()\n",
    "    \n",
    "    return word_ids, token_embeddings\n",
    "\n",
    "def get_substitute_indecies(token_mapping, word_array, substitute_array):\n",
    "    \"\"\"Find the token indecies of the part of the sentence we would\n",
    "        like to find substitutes for\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokenized_sent : list\n",
    "        list of wordpieces\n",
    "    sent: str\n",
    "        input sentence\n",
    "    substitute_phrase: str\n",
    "        part of the sentence to find substitute alternatives for\n",
    "        must be lowercase, if we use bert-base-uncased\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array \n",
    "        token indecies of the substitution phrase\n",
    "\n",
    "    \"\"\"\n",
    "    word_indecies = False\n",
    "    # find word indecies for overlap\n",
    "    for i in range(len(word_array)): \n",
    "        sub_array = word_array[i:i+len(substitute_array)]\n",
    "        if (sub_array == substitute_array): \n",
    "            word_indecies = [i+j for j in range(len(substitute_array))]\n",
    "            break\n",
    "    if (not word_indecies): \n",
    "        return False\n",
    "    \n",
    "    sub_ids = [token_index for word_index in word_indecies for token_index in np.where(token_mapping == word_index)[0]]\n",
    "\n",
    "    return sub_ids\n",
    "\n",
    "def get_substitute_embedding(token_embeddings, substitute_indecies): \n",
    "    return np.mean(token_embeddings[substitute_indecies], axis=0)\n",
    "\n",
    "\n",
    "def find_similar_ngrams(sent, substitute_phrase, top = 10): \n",
    "    \"\"\"Find similar n-grams to the substitute phrase in the\n",
    "        input sentence\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sent : str\n",
    "        input sentence\n",
    "    substitute_phrase: str\n",
    "        subpart of input sentence to find substitutions for\n",
    "    top: int\n",
    "        how many substitution suggestions the program should\n",
    "        give\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array \n",
    "        array of the most similar n-grams to the substitution phrase\n",
    "\n",
    "    \"\"\"\n",
    "    # split into word array, both for sent and for sub-phrase\n",
    "    word_array = word_tokenize(sent)\n",
    "    sub_array = word_tokenize(substitute_phrase)\n",
    "\n",
    "    # find embeddings for sentence\n",
    "    token_mapping, token_embeddings = tokens_to_embeddings(word_array)\n",
    "    token_embeddings = token_embeddings[:, LAYER, :]\n",
    "\n",
    "    # extract relevant phrases for ngram\n",
    "    substitute_indecies = get_substitute_indecies(token_mapping, word_array, sub_array)\n",
    "    substitute_embedding = get_substitute_embedding(token_embeddings, substitute_indecies)\n",
    "    \n",
    "    return word_model.similar_by_vector(substitute_embedding, topn=top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7dce36a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_changes_fast(sent): \n",
    "    word_tokens = nltk.word_tokenize(sent.lower())\n",
    "    # find embeddings of the sentence words\n",
    "    token_mapping, token_embeddings = tokens_to_embeddings(word_tokens)\n",
    "    token_embeddings = token_embeddings[:, LAYER, :]\n",
    "    \n",
    "    for i, token in enumerate(word_tokens): \n",
    "        # extract relevant phrases for ngram\n",
    "        substitute_indecies = get_substitute_indecies(token_mapping, word_tokens, [token])\n",
    "        substitute_embedding = get_substitute_embedding(token_embeddings, substitute_indecies)\n",
    "        sub = word_model.similar_by_vector(substitute_embedding, topn=1)\n",
    "        \n",
    "        # only suggest correction if it does not match the word itself\n",
    "        if (sub[0][0] != token and token not in [\".\", \",\", \"?\", \"-\"]): \n",
    "            word_tokens[i] = word_tokens[i]+\"/(\"+sub[0][0]+\")\"\n",
    "    return \" \".join(word_tokens)\n",
    "\n",
    "def recommend_changes(sent): \n",
    "    sent = sent.lower()\n",
    "    word_tokens = nltk.word_tokenize(sent)\n",
    "    for i, token in enumerate(word_tokens): \n",
    "        sub = find_similar_ngrams(sent, token, top = 1)\n",
    "        if (sub[0][0] != token and token not in [\".\", \",\", \"?\", \"-\"]): \n",
    "            word_tokens[i] = word_tokens[i]+\"/\"+sub[0][0]\n",
    "    return \" \".join(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ac91ba",
   "metadata": {},
   "source": [
    "## Test embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ffebf68b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('codes', 0.7800700664520264),\n",
       " ('script', 0.7725754976272583),\n",
       " ('implementation', 0.7720517516136169),\n",
       " ('coded', 0.7638325691223145),\n",
       " ('suite', 0.7323173880577087),\n",
       " ('architecture', 0.7209630012512207),\n",
       " ('toolkit', 0.7193244695663452),\n",
       " ('api', 0.7193145155906677),\n",
       " ('program', 0.7187706232070923),\n",
       " ('framework', 0.71236652135849)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_model.most_similar(\"code\", topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6af8de75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('unusual', 0.7220069766044617),\n",
       " ('novel', 0.6847960352897644),\n",
       " ('unique', 0.6456605792045593),\n",
       " ('unwanted', 0.6455644369125366),\n",
       " ('amazing', 0.6397463083267212),\n",
       " ('interesting', 0.6375235915184021),\n",
       " ('complex', 0.6298054456710815),\n",
       " ('phenomena', 0.6267231106758118),\n",
       " ('artifacts', 0.6227136850357056),\n",
       " ('complicated', 0.6226660013198853)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sent = \"There were many weird data points in the dataset.\"\n",
    "substitute_phrase = \"weird\"\n",
    "find_similar_ngrams(test_sent, substitute_phrase, top = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8beee42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'there were many weird/(unusual) data points/(point) in the dataset .'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"There were many weird data points in the dataset.\"\n",
    "recommend_changes_fast(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c06c823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sudoku_par = \"\"\"In this chapter we will try and explain how the code implementation works with its SudokuSolver class and functions. We have included some figures to examplify the processes and give an insight to how the program works. To debug the solution we have often printed temporary states to find out where the mistakes occurred. The input format for the sudoku is assumed to be a string where each row comes after one another without spaces and the cells that are not filled are set to 0.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "197513fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in this/(here) chapter/(section) we will try/(examine) and/(to) explain how the code implementation works/(performs) with its sudokusolver/(enchmark) class and functions . we have included some figures/(statistics) to examplify/(simplify) the processes/(process) and give an/(detailed) insight/(understand) to how the program works/(operate) . to debug/(reï¬‚ect) the solution we have often printed temporary/(additional) states to find/(discover) out where/(locations) the mistakes/(errors) occurred/(occurs) . the input format for the sudoku/(heuristic) is assumed to be a string where each row comes/(occurs) after one another without spaces and the cells/(cell) that are not filled/(spaces) are set/(assign) to 0/(zero) .\n"
     ]
    }
   ],
   "source": [
    "print(recommend_changes_fast(sudoku_par))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b30216a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66d5e10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
